{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivelina/anaconda3/envs/DL/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_label_encoding(data, columns, new_column_name = 'new_col', drop_columns = False):\n",
    "    data[new_column_name] = sum([(idx + 1) * data[col_name] for idx, col_name in enumerate(columns)])\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name_start(data, name_start):\n",
    "    filtered_columns = data.columns[data.columns.str.startswith(name_start)]\n",
    "    return filtered_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['meaneduc'] = data['meaneduc'].fillna(data['meaneduc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data['meaneduc'].isna().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['v2a1', 'v18q1', 'rez_esc'], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['edjefe'] = data['edjefe'].replace({'no': 0, 'yes': 1}).astype(float)\n",
    "data['edjefa'] = data['edjefa'].replace({'no': 0, 'yes': 1}).astype(float)\n",
    "data['dependency'] = data['dependency'].replace({'no': 0, 'yes': 1}).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mobilephone_per_person'] = np.where(data['qmobilephone'] > 0, data['qmobilephone'] / data['r4t2'], 0)\n",
    "data['people_per_room'] = data['rooms'] / data['tamviv']\n",
    "data['people_per_bedroom'] = data['bedrooms'] / data['tamviv']\n",
    "data['paying_rent'] = np.where(\n",
    "    (data['tipovivi2'] == 1) | (data['tipovivi3'] == 1) | (data['tipovivi4'] == 1),\n",
    "    1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['idhogar', 'rooms', 'bedrooms', 'qmobilephone', 'mobilephone', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',\n",
    "                 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned',\n",
    "                 'agesq'], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder\n",
    "#### Categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_material_cols = filter_by_name_start(data, 'pared')\n",
    "floor_material_cols = filter_by_name_start(data, 'piso')\n",
    "roof_material_cols = filter_by_name_start(data, 'techo')\n",
    "water_provision_cols = filter_by_name_start(data, 'abastagua')\n",
    "electricity_cols = data[['public', 'planpri', 'noelec', 'coopele']]\n",
    "waste_water_connection_cols = filter_by_name_start(data, 'sanitario')\n",
    "energy_source_for_cooking_cols = filter_by_name_start(data, 'energcocinar')\n",
    "rubbish_disposal_cols = filter_by_name_start(data, 'elimbasu')\n",
    "marital_status_cols = filter_by_name_start(data, 'estadocivil')\n",
    "family_relation_cols = filter_by_name_start(data, 'parentesco')\n",
    "level_of_education_cols = filter_by_name_start(data, 'instlevel')\n",
    "region_cols = filter_by_name_start(data, 'lugar')\n",
    "walls_condition_cols = filter_by_name_start(data, 'epared')\n",
    "roof_condition_cols = filter_by_name_start(data, 'etecho')\n",
    "floor_condition_cols = filter_by_name_start(data, 'eviv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = one_hot_to_label_encoding(data, outside_material_cols, new_column_name = 'outside_material', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, floor_material_cols, new_column_name = 'floor_material', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, roof_material_cols, new_column_name = 'roof_material', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, water_provision_cols, new_column_name = 'water_provisio', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, electricity_cols, new_column_name = 'electricity', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, waste_water_connection_cols, new_column_name = 'waste_water_connection', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, energy_source_for_cooking_cols, new_column_name = 'energy_for_cooking', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, rubbish_disposal_cols, new_column_name = 'rubbish_disposal', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, marital_status_cols, new_column_name = 'marital_status', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, family_relation_cols, new_column_name = 'family_relation', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, level_of_education_cols, new_column_name = 'level_of_education', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, region_cols, new_column_name = 'region', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, walls_condition_cols, new_column_name = 'walls_condition', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, roof_condition_cols, new_column_name = 'roof_condition', drop_columns = True)\n",
    "data = one_hot_to_label_encoding(data, floor_condition_cols, new_column_name = 'floor_condition', drop_columns = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['r4h1', 'r4h2', 'r4m1', 'r4m2', 'r4t1', 'r4t2', 'r4t3',\n",
    "                  'male', 'female', 'hogar_total'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['r4h3', 'r4m3', 'tamhog', 'tamhog', 'escolari', 'hhsize', 'hogar_nin', 'hogar_adul', 'hogar_mayor', 'dependency', \n",
    "                    'edjefe', 'edjefa', 'meaneduc', 'overcrowding', 'age', 'mobilephone_per_person', 'people_per_room', 'people_per_bedroom']\n",
    "categorical_columns = data.columns[~data.columns.isin(numerical_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_col in categorical_columns:\n",
    "    data[cat_col] = data[cat_col].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_original = ['extreme poverty', 'moderate poverty', 'vulnerable households', 'non vulnerable households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Target']\n",
    "attributes = data.drop(['Target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder().fit(target.unique())\n",
    "target_encoded = label_encoder.transform(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split 80/20 for train and test. Because of the imbalance of the classes stratification is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 151,  319,  242, 1200]), array([ 604, 1278,  967, 4796]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attributes, test_attributes, train_target, test_target = train_test_split(attributes, target_encoded, train_size = 0.8, stratify = target_encoded, random_state=13)\n",
    "np.bincount(test_target), np.bincount(train_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(train_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attributes_scaled = scaler.transform(train_attributes)\n",
    "test_attributes_scaled = scaler.transform(test_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000000000002, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_attributes_scaled.max(), train_attributes_scaled.min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "#### Mutual Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a test data is set aside, a feature selection using mutual information classification will be applied. As seen from the result below, the highest value is only 0.158925, which means that the features left have low dependencies. Because of that features won't be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = mutual_info_classif(train_attributes_scaled, train_target)\n",
    "mutual_info = pd.Series(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19    0.152779\n",
       "16    0.098945\n",
       "27    0.096100\n",
       "28    0.073452\n",
       "13    0.065944\n",
       "20    0.062964\n",
       "26    0.062818\n",
       "17    0.058873\n",
       "40    0.057215\n",
       "9     0.055676\n",
       "11    0.054685\n",
       "30    0.046296\n",
       "44    0.043863\n",
       "42    0.043748\n",
       "31    0.040662\n",
       "10    0.038187\n",
       "4     0.036995\n",
       "43    0.033803\n",
       "18    0.033435\n",
       "14    0.031943\n",
       "38    0.028726\n",
       "8     0.027758\n",
       "5     0.024909\n",
       "37    0.024599\n",
       "21    0.024546\n",
       "35    0.023256\n",
       "41    0.022888\n",
       "36    0.019760\n",
       "22    0.019425\n",
       "25    0.018718\n",
       "0     0.017326\n",
       "7     0.017251\n",
       "24    0.016604\n",
       "6     0.016209\n",
       "29    0.014572\n",
       "23    0.010186\n",
       "39    0.008241\n",
       "3     0.006733\n",
       "33    0.005652\n",
       "2     0.003159\n",
       "32    0.000000\n",
       "34    0.000000\n",
       "1     0.000000\n",
       "15    0.000000\n",
       "12    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info.sort_values(ascending = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A XGB classifier is used for the task. The tuned parameters are as follows:\n",
    "- `gamma` (Minimum loss reduction required to make a further partition on a leaf node of the tree.) - 1\n",
    "- `min_child_weight` (Minimum sum of instance weight (hessian) needed in a child.) - 5\n",
    "- `learning_rate` - 0.08\n",
    "- `max_depth` - 7\n",
    "- `n_estimators` - 300\n",
    "- `max_delta_step` (Maximum delta step we allow each leaf output to be.) - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = xgb.XGBClassifier(objective = 'multi:softmax',\n",
    "                                num_class=4,\n",
    "                                gamma = 1,\n",
    "                                min_child_weight = 5,\n",
    "                                learning_rate = 0.08,\n",
    "                                max_depth = 7,\n",
    "                                n_estimators = 300,\n",
    "                                max_delta_step = 4,\n",
    "                                eval_metric = 'merror',\n",
    "                                use_label_encoder = False,\n",
    "                                seed = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='merror', gamma=1, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.08, max_delta_step=4,\n",
       "              max_depth=7, min_child_weight=5, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
       "              num_class=4, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=13, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=None, seed=13, subsample=1, tree_method='exact', ...)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.fit(train_attributes_scaled, train_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97       604\n",
      "           1       0.98      0.94      0.96      1278\n",
      "           2       0.97      0.92      0.95       967\n",
      "           3       0.97      1.00      0.99      4796\n",
      "\n",
      "    accuracy                           0.97      7645\n",
      "   macro avg       0.98      0.95      0.96      7645\n",
      "weighted avg       0.97      0.97      0.97      7645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_target, tuned_model.predict(train_attributes_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.82       151\n",
      "           1       0.89      0.80      0.84       319\n",
      "           2       0.92      0.71      0.80       242\n",
      "           3       0.91      0.99      0.95      1200\n",
      "\n",
      "    accuracy                           0.90      1912\n",
      "   macro avg       0.90      0.82      0.85      1912\n",
      "weighted avg       0.90      0.90      0.90      1912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_target, tuned_model.predict(test_attributes_scaled)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the results below, the recall is lower that the precision for more of the classes. This means that the model gives some false negative. \n",
    "\n",
    "Lowest f-1 score is present for class 2 (vulnerable households), despite the fact that it's not the class with least samples, which leads to the conclusion that it's the hardest to distinguish. \n",
    "\n",
    "The highest f-1 score of 95% is for class 4 (non-vulnerable households), which also has most of the records in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
